{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "katibumoja",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EtTNW8coFGt"
      },
      "source": [
        "TRAINER_FILE = \"umoja.py\"\n",
        "KUBERNETES_FILE = \"umoja-xgb.yaml\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD7oXu3Rm7R-",
        "outputId": "93f04c59-6518-41cd-c5d3-d00f2a92dc34"
      },
      "source": [
        "%%writefile $TRAINER_FILE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import argparse\n",
        "import logging\n",
        "import warnings\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=SettingWithCopyWarning)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "def prep_data():\n",
        "  # Load data\n",
        "  all_data=pd.read_csv('https://raw.githubusercontent.com/Josepholaidepetro/Umojahack/main/maven/Train.csv')\n",
        "  print(\"all_data size is : {}\".format(all_data.shape))\n",
        "  return all_data\n",
        "\n",
        "def preprocess():\n",
        "  # call the function\n",
        "  all_data = prep_data()\n",
        "  # Convert date columns to datetime datatypes \n",
        "  for i in all_data.columns:\n",
        "    if i[-4:] == 'Date':\n",
        "      all_data[str(i)] = pd.to_datetime(all_data[str(i)],infer_datetime_format=True, errors='coerce')\n",
        "\n",
        "  # noticed some strange occurence in the age column, as regarding the max and min\n",
        "  # pre-processing the age column\n",
        "  all_data['Age'].loc[all_data['Age'] < 0] = all_data['Age'].loc[all_data['Age'] < 0] * -1\n",
        "  all_data['Age'] = np.where(all_data['Age'] == 320, 120, all_data['Age'])\n",
        "  all_data['Age'] = np.where(all_data['Age'] > 320, 99, all_data['Age'])\n",
        "  return all_data\n",
        "\n",
        "def extract_date_info():\n",
        "  # call the function\n",
        "  all_data = preprocess()\n",
        "\n",
        "  # Extract Date features\n",
        "  date_col = ['Policy Start Date', 'Policy End Date', 'First Transaction Date']\n",
        "\n",
        "  all_data['Date diff'] = (all_data['Policy End Date'].dt.year - all_data['Policy Start Date'].dt.year) * 12 \\\n",
        "  + (all_data['Policy End Date'].dt.month - all_data['Policy Start Date'].dt.month)\n",
        "  for feat in date_col:\n",
        "      all_data[feat +'_day'] = all_data[feat].dt.day\n",
        "      all_data[feat +'_month'] = all_data[feat].dt.month\n",
        "      all_data[feat +'_quarter'] = all_data[feat].dt.quarter\n",
        "  all_data.drop(columns=date_col,axis=1,inplace=True)\n",
        "  return all_data\n",
        "\n",
        "def deal_missing_data():\n",
        "  all_data = extract_date_info()\n",
        "  # copy data\n",
        "  all_data1 = all_data.copy()\n",
        "\n",
        "  # categorical and continuous features\n",
        "  cat_feat = all_data1.select_dtypes(exclude = np.number).columns\n",
        "  num_feat = all_data1.select_dtypes(exclude = object).columns\n",
        "\n",
        "  # Deal with missing values\n",
        "  for col in num_feat:\n",
        "    if col != 'target':\n",
        "      all_data1[col].fillna(-999, inplace = True)  \n",
        "      \n",
        "  for col in cat_feat:\n",
        "    all_data1[col].fillna('NONE', inplace = True)\n",
        "    \n",
        "  return all_data1\n",
        "\n",
        "def feat_eng():\n",
        "  all_data1 = deal_missing_data()\n",
        "  all_data1['LGA_Name'] = all_data1['LGA_Name'].map(all_data1['LGA_Name'].value_counts().to_dict())\n",
        "  all_data1['State'] = all_data1['State'].map(all_data1['State'].value_counts().to_dict())\n",
        "  all_data1['Subject_Car_Make'] = all_data1['Subject_Car_Make'].map(all_data1['Subject_Car_Make'].value_counts().to_dict())\n",
        "  all_data1['Subject_Car_Colour'] = all_data1['Subject_Car_Colour'].map(all_data1['Subject_Car_Colour'].value_counts().to_dict()) \n",
        "  mapper = {\"Male\":\"M\",\"Female\":'F','Entity':'O','Joint Gender':'O',None:'O','NO GENDER':'O','NOT STATED':'O','SEX':'O', np.nan: 'O' }\n",
        "  all_data1.Gender = all_data1.Gender.map(mapper)\n",
        "  return all_data1\n",
        "\n",
        "def encode_var():\n",
        "  all_data1 = feat_eng()\n",
        "  for i in ['ProductName', 'Car_Category']:\n",
        "    encoder = LabelEncoder()\n",
        "    all_data1[str(i)] = encoder.fit_transform(all_data1[str(i)])\n",
        "\n",
        "  # feat engineering with the encoded variable\n",
        "  all_data1['no_pol_prod_name'] = all_data1['No_Pol'] + all_data1['ProductName']\n",
        "\n",
        "  # drop columns\n",
        "  all_data1.drop(columns=['ID', 'Subject_Car_Colour'],inplace=True)\n",
        "  # convert columns with categorical columns to numbers\n",
        "  all_data1=pd.get_dummies(all_data1)\n",
        "  return all_data1\n",
        "\n",
        "def modelling_data():\n",
        "  all_data1 = encode_var()\n",
        "\n",
        "  #Get the train dataset\n",
        "  train_n = all_data1.copy()\n",
        "  target = 'target'\n",
        "  features = [c for c in train_n.columns if c not in ['target']]\n",
        "\n",
        "  scores = 0\n",
        "  k = 5\n",
        "  kf = StratifiedKFold(k)\n",
        "\n",
        "  for i, (tr_idx, vr_idx) in enumerate(kf.split(train_n, train_n[target])):\n",
        "      xtrain, ytrain = train_n.loc[tr_idx, features], train_n.loc[tr_idx, target]\n",
        "      xval, yval = train_n.loc[vr_idx, features], train_n.loc[vr_idx, target]\n",
        "      \n",
        "    # training and validation\n",
        "      model=XGBClassifier(scale_pos_weight=args.scale_pos_weight, \n",
        "                          max_depth=args.max_depth,\n",
        "                          learning_rate=args.learning_rate, \n",
        "                          n_estimators=args.n_estimators, \n",
        "                          silent=True,\n",
        "                          metrics='auc',\n",
        "                          colsample_bylevel=args.colsample_bylevel,\n",
        "                          reg_alpha=args.reg_alpha)\n",
        "      model.fit(xtrain, ytrain, eval_set=[(xval,yval)], early_stopping_rounds=100,verbose=100)\n",
        "      pred = model.predict(xval)\n",
        "\n",
        "      #predicting on test set\n",
        "      score = roc_auc_score(yval, pred)\n",
        "      #\n",
        "      scores += score/k\n",
        "\n",
        "  roc_auc_scores = scores\n",
        "  return roc_auc_scores\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--scale_pos_weight',\n",
        "                      type = float,\n",
        "                      default = 8.1922929,\n",
        "                      help = 'Control the balance of positive and negative weights, useful for unbalanced classes.')\n",
        "  parser.add_argument('--colsample_bylevel',\n",
        "                      type = float,\n",
        "                      default = 0.8,\n",
        "                      help = 'the subsample ratio of columns for each level.')\n",
        "  parser.add_argument('--learning_rate',\n",
        "                      type = float,\n",
        "                      default = 0.143242,\n",
        "                      help = 'Step size shrinkage used in update to prevent overfitting.')\n",
        "  parser.add_argument('--max-depth',\n",
        "                      type = int,\n",
        "                      default = 10,\n",
        "                      help = 'Maximum depth of a tree.')\n",
        "  parser.add_argument('--n_estimators',\n",
        "                      type = int,\n",
        "                      default = 800 ,\n",
        "                      help = 'Number of trees to fit.')\n",
        "  parser.add_argument('--reg_alpha',\n",
        "                      type = float,\n",
        "                      default = 0.8,\n",
        "                      help = 'L1 regularization term on weights.')\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  prep_data()\n",
        "  preprocess()\n",
        "  extract_date_info()\n",
        "  deal_missing_data()\n",
        "  feat_eng()\n",
        "  encode_var()\n",
        "  roc_auc_score = modelling_data()\n",
        "\n",
        "  logging.info(\"accuracy={:.4f}\".format(roc_auc_score))\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting umoja.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCKYeEWwB1zc",
        "outputId": "c1cfad19-ddfd-40d5-802e-1422f9aba7ef"
      },
      "source": [
        "%run $TRAINER_FILE --reg_alpha 0.8"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data size is : (12079, 14)\n",
            "all_data size is : (12079, 14)\n",
            "all_data size is : (12079, 14)\n",
            "all_data size is : (12079, 14)\n",
            "all_data size is : (12079, 14)\n",
            "all_data size is : (12079, 14)\n",
            "all_data size is : (12079, 14)\n",
            "[0]\tvalidation_0-error:0.326987\n",
            "Will train until validation_0-error hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-error:0.19495\n",
            "[200]\tvalidation_0-error:0.181705\n",
            "[300]\tvalidation_0-error:0.173427\n",
            "Stopping. Best iteration:\n",
            "[277]\tvalidation_0-error:0.172599\n",
            "\n",
            "[0]\tvalidation_0-error:0.341887\n",
            "Will train until validation_0-error hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-error:0.177566\n",
            "[200]\tvalidation_0-error:0.165977\n",
            "[300]\tvalidation_0-error:0.161424\n",
            "[400]\tvalidation_0-error:0.161424\n",
            "[500]\tvalidation_0-error:0.161424\n",
            "Stopping. Best iteration:\n",
            "[403]\tvalidation_0-error:0.160596\n",
            "\n",
            "[0]\tvalidation_0-error:0.3125\n",
            "Will train until validation_0-error hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-error:0.17053\n",
            "[200]\tvalidation_0-error:0.156871\n",
            "[300]\tvalidation_0-error:0.152732\n",
            "[400]\tvalidation_0-error:0.15149\n",
            "Stopping. Best iteration:\n",
            "[316]\tvalidation_0-error:0.150662\n",
            "\n",
            "[0]\tvalidation_0-error:0.33899\n",
            "Will train until validation_0-error hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-error:0.177566\n",
            "[200]\tvalidation_0-error:0.170944\n",
            "[300]\tvalidation_0-error:0.166805\n",
            "Stopping. Best iteration:\n",
            "[287]\tvalidation_0-error:0.166805\n",
            "\n",
            "[0]\tvalidation_0-error:0.344513\n",
            "Will train until validation_0-error hasn't improved in 100 rounds.\n",
            "[100]\tvalidation_0-error:0.175155\n",
            "[200]\tvalidation_0-error:0.158178\n",
            "[300]\tvalidation_0-error:0.156936\n",
            "Stopping. Best iteration:\n",
            "[272]\tvalidation_0-error:0.156522\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:accuracy=0.6324\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VstzUzB3CIMY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}